{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'shakespeare.txt'\n",
    "with open(inputFile) as f:\n",
    "  text = f.read()\n",
    "  print('length of dataset in characters: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# this is the vocabulary i.e. all teh unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers (i.e. encoding and decoding)\n",
    "# this is needed as the tensors work with numbers\n",
    "\n",
    "# create two lookup dictionaries from char to int, and vice versa\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "test = encode('Hello World!')\n",
    "print(test)\n",
    "print(decode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the whole input text and store it as a tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into training and valdiation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 8\n",
    "train_data[:block_size + 1] # we need +1 to get 8 training sets\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # the training input\n",
    "y = train_data[1:block_size + 1] # the resulting test (one offset compared to train)\n",
    "for t in range(block_size):\n",
    "  context = x[:t + 1]\n",
    "  target = y[t]\n",
    "  print(f'when input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[46, 39, 58,  1, 51, 47, 57, 41],\n",
      "        [39, 47, 42,  0, 20, 47, 57,  1],\n",
      "        [ 1, 42, 43, 39, 58, 46,  6,  1],\n",
      "        [59, 50, 58, 12,  0, 21, 58,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 58,  1, 51, 47, 57, 41, 39],\n",
      "        [47, 42,  0, 20, 47, 57,  1, 58],\n",
      "        [42, 43, 39, 58, 46,  6,  1, 39],\n",
      "        [50, 58, 12,  0, 21, 58,  1, 52]])\n",
      "-------\n",
      "for context [46] the target is 39\n",
      "for context [46, 39] the target is 58\n",
      "for context [46, 39, 58] the target is 1\n",
      "for context [46, 39, 58, 1] the target is 51\n",
      "for context [46, 39, 58, 1, 51] the target is 47\n",
      "for context [46, 39, 58, 1, 51, 47] the target is 57\n",
      "for context [46, 39, 58, 1, 51, 47, 57] the target is 41\n",
      "for context [46, 39, 58, 1, 51, 47, 57, 41] the target is 39\n",
      "for context [39] the target is 47\n",
      "for context [39, 47] the target is 42\n",
      "for context [39, 47, 42] the target is 0\n",
      "for context [39, 47, 42, 0] the target is 20\n",
      "for context [39, 47, 42, 0, 20] the target is 47\n",
      "for context [39, 47, 42, 0, 20, 47] the target is 57\n",
      "for context [39, 47, 42, 0, 20, 47, 57] the target is 1\n",
      "for context [39, 47, 42, 0, 20, 47, 57, 1] the target is 58\n",
      "for context [1] the target is 42\n",
      "for context [1, 42] the target is 43\n",
      "for context [1, 42, 43] the target is 39\n",
      "for context [1, 42, 43, 39] the target is 58\n",
      "for context [1, 42, 43, 39, 58] the target is 46\n",
      "for context [1, 42, 43, 39, 58, 46] the target is 6\n",
      "for context [1, 42, 43, 39, 58, 46, 6] the target is 1\n",
      "for context [1, 42, 43, 39, 58, 46, 6, 1] the target is 39\n",
      "for context [59] the target is 50\n",
      "for context [59, 50] the target is 58\n",
      "for context [59, 50, 58] the target is 12\n",
      "for context [59, 50, 58, 12] the target is 0\n",
      "for context [59, 50, 58, 12, 0] the target is 21\n",
      "for context [59, 50, 58, 12, 0, 21] the target is 58\n",
      "for context [59, 50, 58, 12, 0, 21, 58] the target is 1\n",
      "for context [59, 50, 58, 12, 0, 21, 58, 1] the target is 52\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13371337)\n",
    "batch_count = 4  # how many sequences will be processed in time i.e. how much training can we do in parallell afaik\n",
    "block_size = 8 # how large is the context i.e. how many chars can we look back at and use in our prediction\n",
    "\n",
    "def get_batches(data, block_size, batch_count):\n",
    "  ix = torch.randint(len(data) - block_size, (batch_count,)) \n",
    "  x = torch.stack([data[i:i + block_size] for i in ix]) # training input\n",
    "  y = torch.stack([data[i + 1:i+block_size + 1] for i in ix]) # target output\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batches(train_data, block_size, batch_count)\n",
    "print('inputs: ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-------')\n",
    "\n",
    "for b in range(batch_count):\n",
    "  for t in range(block_size):\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f'for context {context.tolist()} the target is {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "expected loss 4.174387269895637\n",
      "tensor(4.7554, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "CvOXxfSWjg--,aavU.fsW3jsoq:IH!U!RFPsUc:OHOnSVBMulr;ReeeHPT&Uv'MOE,s;oBgFZqee.3gJ:SKVTudIZqfCFfHOzWfC\n"
     ]
    }
   ],
   "source": [
    "#create the bigram language model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(13371337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, contexts, targets=None):\n",
    "\n",
    "    # we make a prediction based on a single token\n",
    "    logits = self.token_embedding_table(contexts) # Batch Time Channel tensor (BTC)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "    \n",
    "    # we have provided targets so compute the loss\n",
    "    # we need to reshape the data for use in cross entropy\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B * T, C)\n",
    "    targets = targets.view(B * T)\n",
    "\n",
    "    # compare to the target using neg log likeliehood loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of incdices into the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self.forward(idx)\n",
    "      logits = logits[:, -1, :] # pick the last element from the time dimension B, _T_, C\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # samples from the probabilities\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # add the prediction to the time dimension\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m.forward(xb, yb)\n",
    "print(out.shape)\n",
    "print(\"expected loss\", -math.log(1/vocab_size))\n",
    "print(loss)\n",
    "\n",
    "# lets generate some text\n",
    "# this text will be completely random as the model is not trained\n",
    "newline = torch.zeros((1, 1), dtype=torch.long)\n",
    "output = m.generate(idx = newline, max_new_tokens=100)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, data, block_size, batch_count, eval_iters):\n",
    "  out = {}\n",
    "  model.eval()\n",
    "  losses = torch.zeros(eval_iters)\n",
    "  for k in range(eval_iters):\n",
    "    X, Y = get_batches(data, block_size, batch_count)\n",
    "    logits, loss = model.forward(X, Y)\n",
    "    losses[k] = loss.item()\n",
    "  model.train() # why this one?\n",
    "  return losses.mean()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4711, val loss 2.4925\n",
      "step 200: train loss 2.4666, val loss 2.4829\n",
      "step 400: train loss 2.4649, val loss 2.4882\n",
      "step 600: train loss 2.4632, val loss 2.4874\n",
      "step 800: train loss 2.4623, val loss 2.4859\n",
      "step 1000: train loss 2.4580, val loss 2.4762\n",
      "step 1200: train loss 2.4678, val loss 2.4924\n",
      "step 1400: train loss 2.4577, val loss 2.4813\n",
      "step 1600: train loss 2.4536, val loss 2.4793\n",
      "step 1800: train loss 2.4573, val loss 2.4856\n",
      "step 2000: train loss 2.4608, val loss 2.4865\n",
      "step 2200: train loss 2.4549, val loss 2.4756\n",
      "step 2400: train loss 2.4601, val loss 2.4879\n",
      "step 2600: train loss 2.4559, val loss 2.4792\n",
      "step 2800: train loss 2.4581, val loss 2.4890\n",
      "step 3000: train loss 2.4533, val loss 2.4860\n",
      "step 3200: train loss 2.4576, val loss 2.4857\n",
      "step 3400: train loss 2.4529, val loss 2.4829\n",
      "step 3600: train loss 2.4554, val loss 2.4872\n",
      "step 3800: train loss 2.4549, val loss 2.4828\n",
      "step 4000: train loss 2.4544, val loss 2.4749\n",
      "step 4200: train loss 2.4706, val loss 2.4893\n",
      "step 4400: train loss 2.4547, val loss 2.4748\n",
      "step 4600: train loss 2.4523, val loss 2.4863\n",
      "step 4800: train loss 2.4530, val loss 2.4778\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_data, val_data):\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "  batch_size = 32\n",
    "  max_iters = 5000\n",
    "  eval_iters = 200\n",
    "  eval_interval = 200\n",
    "  for steps in range(max_iters):\n",
    "    xb, yb = get_batches(train_data, block_size, batch_size)\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "      train_loss = estimate_loss(model, train_data, block_size, batch_size, eval_iters)\n",
    "      val_loss = estimate_loss(model, val_data, block_size, batch_size, eval_iters)\n",
    "      print(f\"step {steps}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(m, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RD:\n",
      "Bursowers, m, qkethe thememe? F goug wid pr?\n",
      "Be be k bet thithispy did:\n",
      "Tho\n",
      "AS pe:\n",
      "I\n",
      "ps PRY authatwheleind ckntxTCENBO; wh oZEO:\n",
      "Thase by CHe Icorinavelur edrdimar' s 'l us, befrd tr s wooamonthe \n"
     ]
    }
   ],
   "source": [
    "newline = torch.zeros((1, 1), dtype=torch.long)\n",
    "output = m.generate(idx = newline, max_new_tokens=200)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now add some attention. This is done by using the average of the previous token values\n",
    "# this is a simplification and much information is lost but better than bi-gram\n",
    "\n",
    "class BigramLanguageModel_v2(nn.Module):\n",
    "  def __init__(self, vocab_size, block_size, n_embed):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "  def forward(self, contexts, targets=None):\n",
    "    B, T = contexts.shape\n",
    "\n",
    "    tok_emb = self.token_embedding_table(contexts)\n",
    "    pos_emb = self.position_embedding_table(torch.arange(T, device='cpu'))\n",
    "    x = tok_emb + pos_emb\n",
    "\n",
    "    logits = self.lm_head(x) # Batch Time Channel tensor (BTC)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "    \n",
    "    # we have provided targets so compute the loss\n",
    "    # we need to reshape the data for use in cross entropy\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B * T, C)\n",
    "    targets = targets.view(B * T)\n",
    "\n",
    "    # compare to the target using neg log likeliehood loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of incdices into the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:, -block_size:]\n",
    "      logits, loss = self.forward(idx_cond)\n",
    "      logits = logits[:, -1, :] # pick the last element from the time dimension B, _T_, C\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # samples from the probabilities\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # add the prediction to the time dimension\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4758, val loss 4.4705\n",
      "step 200: train loss 3.0527, val loss 3.0695\n",
      "step 400: train loss 2.7814, val loss 2.8115\n",
      "step 600: train loss 2.6903, val loss 2.7051\n",
      "step 800: train loss 2.6368, val loss 2.6456\n",
      "step 1000: train loss 2.5944, val loss 2.6118\n",
      "step 1200: train loss 2.5682, val loss 2.5878\n",
      "step 1400: train loss 2.5571, val loss 2.5770\n",
      "step 1600: train loss 2.5508, val loss 2.5612\n",
      "step 1800: train loss 2.5339, val loss 2.5562\n",
      "step 2000: train loss 2.5276, val loss 2.5470\n",
      "step 2200: train loss 2.5289, val loss 2.5360\n",
      "step 2400: train loss 2.5085, val loss 2.5226\n",
      "step 2600: train loss 2.5164, val loss 2.5260\n",
      "step 2800: train loss 2.4999, val loss 2.5350\n",
      "step 3000: train loss 2.5026, val loss 2.5162\n",
      "step 3200: train loss 2.4990, val loss 2.5193\n",
      "step 3400: train loss 2.5029, val loss 2.5206\n",
      "step 3600: train loss 2.5028, val loss 2.5049\n",
      "step 3800: train loss 2.4894, val loss 2.5032\n",
      "step 4000: train loss 2.4940, val loss 2.5082\n",
      "step 4200: train loss 2.4909, val loss 2.5209\n",
      "step 4400: train loss 2.4849, val loss 2.5053\n",
      "step 4600: train loss 2.4916, val loss 2.5029\n",
      "step 4800: train loss 2.4835, val loss 2.5055\n"
     ]
    }
   ],
   "source": [
    "mv2 = BigramLanguageModel_v2(vocab_size, block_size, 32)\n",
    "train_model(mv2, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BRK:\n",
      "\n",
      "Fak foanisand c'llle s,\n",
      "\n",
      "WELA!\n",
      "\n",
      "\n",
      "Andanpre my lhe eseseve my ed terd s, thimy or erins d lllth tht-e we t sherhoug bored thf eathere errsefy hede tht wen, co l d ththithe'dy bap ll t ME n n he t\n"
     ]
    }
   ],
   "source": [
    "output = mv2.generate(idx = newline, max_new_tokens=200)\n",
    "print(decode(output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
