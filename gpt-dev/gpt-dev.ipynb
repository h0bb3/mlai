{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'shakespeare.txt'\n",
    "with open(inputFile) as f:\n",
    "  text = f.read()\n",
    "  print('length of dataset in characters: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# this is the vocabulary i.e. all the unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers (i.e. encoding and decoding)\n",
    "# this is needed as the tensors work with numbers\n",
    "\n",
    "# create two lookup dictionaries from char to int, and vice versa\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "test = encode('Hello World!')\n",
    "print(test)\n",
    "print(decode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the whole input text and store it as a tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into training and valdiation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 8\n",
    "train_data[:block_size + 1] # we need +1 to get 8 training sets\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # the training input\n",
    "y = train_data[1:block_size + 1] # the resulting test (one offset compared to train)\n",
    "for t in range(block_size):\n",
    "  context = x[:t + 1]\n",
    "  target = y[t]\n",
    "  print(f'when input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[46, 39, 58,  1, 51, 47, 57, 41],\n",
      "        [39, 47, 42,  0, 20, 47, 57,  1],\n",
      "        [ 1, 42, 43, 39, 58, 46,  6,  1],\n",
      "        [59, 50, 58, 12,  0, 21, 58,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 58,  1, 51, 47, 57, 41, 39],\n",
      "        [47, 42,  0, 20, 47, 57,  1, 58],\n",
      "        [42, 43, 39, 58, 46,  6,  1, 39],\n",
      "        [50, 58, 12,  0, 21, 58,  1, 52]])\n",
      "-------\n",
      "for context [46] the target is 39\n",
      "for context [46, 39] the target is 58\n",
      "for context [46, 39, 58] the target is 1\n",
      "for context [46, 39, 58, 1] the target is 51\n",
      "for context [46, 39, 58, 1, 51] the target is 47\n",
      "for context [46, 39, 58, 1, 51, 47] the target is 57\n",
      "for context [46, 39, 58, 1, 51, 47, 57] the target is 41\n",
      "for context [46, 39, 58, 1, 51, 47, 57, 41] the target is 39\n",
      "for context [39] the target is 47\n",
      "for context [39, 47] the target is 42\n",
      "for context [39, 47, 42] the target is 0\n",
      "for context [39, 47, 42, 0] the target is 20\n",
      "for context [39, 47, 42, 0, 20] the target is 47\n",
      "for context [39, 47, 42, 0, 20, 47] the target is 57\n",
      "for context [39, 47, 42, 0, 20, 47, 57] the target is 1\n",
      "for context [39, 47, 42, 0, 20, 47, 57, 1] the target is 58\n",
      "for context [1] the target is 42\n",
      "for context [1, 42] the target is 43\n",
      "for context [1, 42, 43] the target is 39\n",
      "for context [1, 42, 43, 39] the target is 58\n",
      "for context [1, 42, 43, 39, 58] the target is 46\n",
      "for context [1, 42, 43, 39, 58, 46] the target is 6\n",
      "for context [1, 42, 43, 39, 58, 46, 6] the target is 1\n",
      "for context [1, 42, 43, 39, 58, 46, 6, 1] the target is 39\n",
      "for context [59] the target is 50\n",
      "for context [59, 50] the target is 58\n",
      "for context [59, 50, 58] the target is 12\n",
      "for context [59, 50, 58, 12] the target is 0\n",
      "for context [59, 50, 58, 12, 0] the target is 21\n",
      "for context [59, 50, 58, 12, 0, 21] the target is 58\n",
      "for context [59, 50, 58, 12, 0, 21, 58] the target is 1\n",
      "for context [59, 50, 58, 12, 0, 21, 58, 1] the target is 52\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13371337)\n",
    "batch_count = 4  # how many sequences will be processed in time i.e. how much training can we do in parallell afaik\n",
    "block_size = 8 # how large is the context i.e. how many chars can we look back at and use in our prediction\n",
    "\n",
    "def get_batches(data, block_size, batch_count):\n",
    "  ix = torch.randint(len(data) - block_size, (batch_count,)) \n",
    "  x = torch.stack([data[i:i + block_size] for i in ix]) # training input\n",
    "  y = torch.stack([data[i + 1:i+block_size + 1] for i in ix]) # target output\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batches(train_data, block_size, batch_count)\n",
    "print('inputs: ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-------')\n",
    "\n",
    "for b in range(batch_count):\n",
    "  for t in range(block_size):\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f'for context {context.tolist()} the target is {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFromNewLine(model, num_new_tokens):\n",
    "  newline = torch.zeros((1, 1), dtype=torch.long)\n",
    "  output = model.generate(idx = newline, max_new_tokens=num_new_tokens)\n",
    "  print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "expected loss 4.174387269895637\n",
      "tensor(4.7554, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "PB\n",
      "s'bLVkUtSQo&Fbn:b;U\n",
      "tDB 3g.Ls?F:Iu'vLeAmXBrL$J?InfhGrh,f3L$ l:skEO,3bHsN.b'XXnkSSS'b&XKwThvv?;ffSgPs.upzWQ FU;XChmK!RFDKrFQpS:H!oLewTQ;&xCL JZWQrFRqgInfnZ3-jgz..jgAbwpeAjm,:zbVRFfcW?IkAF XBtlkBfljPa\n",
      "CzTAaos'xKyBFUAkKVtQ$ brkw$BmBmTnQtynS'CG,cd.Mt \n"
     ]
    }
   ],
   "source": [
    "#create the bigram language model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(13371337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, contexts, targets=None):\n",
    "\n",
    "    # we make a prediction based on a single token\n",
    "    logits = self.token_embedding_table(contexts) # Batch Time Channel tensor (BTC)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "    \n",
    "    # we have provided targets so compute the loss\n",
    "    # we need to reshape the data for use in cross entropy\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B * T, C)\n",
    "    targets = targets.view(B * T)\n",
    "\n",
    "    # compare to the target using neg log likeliehood loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of incdices into the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self.forward(idx)\n",
    "      logits = logits[:, -1, :] # pick the last element from the time dimension B, _T_, C\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # samples from the probabilities\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # add the prediction to the time dimension\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m.forward(xb, yb)\n",
    "print(out.shape)\n",
    "print(\"expected loss\", -math.log(1/vocab_size))\n",
    "print(loss)\n",
    "\n",
    "generateFromNewLine(m, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, data, block_size, batch_count, eval_iters):\n",
    "  out = {}\n",
    "  model.eval()\n",
    "  losses = torch.zeros(eval_iters)\n",
    "  for k in range(eval_iters):\n",
    "    X, Y = get_batches(data, block_size, batch_count)\n",
    "    logits, loss = model.forward(X, Y)\n",
    "    losses[k] = loss.item()\n",
    "  model.train() # why this one?\n",
    "  return losses.mean()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, max_iters):\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "  batch_size = 32\n",
    "  eval_iters = 200\n",
    "  eval_interval = 200\n",
    "  for steps in range(max_iters):\n",
    "    xb, yb = get_batches(train_data, block_size, batch_size)\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "      train_loss = estimate_loss(model, train_data, block_size, batch_size, eval_iters)\n",
    "      val_loss = estimate_loss(model, val_data, block_size, batch_size, eval_iters)\n",
    "      print(f\"step {steps}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7470, val loss 4.7720\n",
      "step 200: train loss 4.5140, val loss 4.5406\n",
      "step 400: train loss 4.3034, val loss 4.3282\n",
      "step 600: train loss 4.0961, val loss 4.1307\n",
      "step 800: train loss 3.9225, val loss 3.9445\n",
      "step 1000: train loss 3.7580, val loss 3.7775\n",
      "step 1200: train loss 3.6149, val loss 3.6365\n",
      "step 1400: train loss 3.4655, val loss 3.5047\n",
      "step 1600: train loss 3.3446, val loss 3.3821\n",
      "step 1800: train loss 3.2391, val loss 3.2717\n",
      "step 2000: train loss 3.1540, val loss 3.1771\n",
      "step 2200: train loss 3.0643, val loss 3.0943\n",
      "step 2400: train loss 2.9880, val loss 3.0194\n",
      "step 2600: train loss 2.9280, val loss 2.9461\n",
      "step 2800: train loss 2.8676, val loss 2.8924\n",
      "step 3000: train loss 2.8269, val loss 2.8443\n",
      "step 3200: train loss 2.7841, val loss 2.8031\n",
      "step 3400: train loss 2.7382, val loss 2.7542\n"
     ]
    }
   ],
   "source": [
    "train_model(m, train_data, val_data, 3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WIut?YO MNaIV:\n",
      "r, ly,-uesUjENu:me C\n",
      "WjeleP thHANAchenges,:elly b myTly p&:Znalyof mo!-ONunBCHd thid os-:s wise Ijesum foo d\n",
      "mifsectLLxVy y tZd trorS\n",
      "KE H'j!:\n",
      "Ifenkis FST;sit oL3orAscN,XGokipe&NGu\n",
      "And pamerngell;\n",
      "\n",
      "HNas I Iker se\n",
      "Inorcr athytr!-Hure, L\n"
     ]
    }
   ],
   "source": [
    "generateFromNewLine(m, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now add some attention. This is done by using the average of the previous token values\n",
    "# this is a simplification and much information is lost but better than bi-gram\n",
    "\n",
    "class BigramLanguageModel_v2(nn.Module):\n",
    "  def __init__(self, vocab_size, block_size, n_embed):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "  def forward(self, contexts, targets=None):\n",
    "    B, T = contexts.shape\n",
    "\n",
    "    tok_emb = self.token_embedding_table(contexts)\n",
    "    pos_emb = self.position_embedding_table(torch.arange(T, device='cpu'))\n",
    "    x = tok_emb + pos_emb\n",
    "\n",
    "    logits = self.lm_head(x) # Batch Time Channel tensor (BTC)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "    \n",
    "    # we have provided targets so compute the loss\n",
    "    # we need to reshape the data for use in cross entropy\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B * T, C)\n",
    "    targets = targets.view(B * T)\n",
    "\n",
    "    # compare to the target using neg log likeliehood loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of incdices into the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:, -block_size:]\n",
    "      logits, loss = self.forward(idx_cond)\n",
    "      logits = logits[:, -1, :] # pick the last element from the time dimension B, _T_, C\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # samples from the probabilities\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # add the prediction to the time dimension\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4447, val loss 4.4538\n",
      "step 200: train loss 3.0448, val loss 3.0806\n",
      "step 400: train loss 2.7923, val loss 2.8138\n",
      "step 600: train loss 2.6951, val loss 2.7175\n",
      "step 800: train loss 2.6318, val loss 2.6422\n",
      "step 1000: train loss 2.5898, val loss 2.6273\n",
      "step 1200: train loss 2.5729, val loss 2.5932\n",
      "step 1400: train loss 2.5491, val loss 2.5716\n",
      "step 1600: train loss 2.5370, val loss 2.5650\n",
      "step 1800: train loss 2.5245, val loss 2.5360\n",
      "step 2000: train loss 2.5303, val loss 2.5394\n",
      "step 2200: train loss 2.5161, val loss 2.5195\n",
      "step 2400: train loss 2.5093, val loss 2.5247\n",
      "step 2600: train loss 2.5017, val loss 2.5208\n",
      "step 2800: train loss 2.4922, val loss 2.5177\n",
      "step 3000: train loss 2.5029, val loss 2.5093\n",
      "step 3200: train loss 2.4925, val loss 2.5099\n",
      "step 3400: train loss 2.5032, val loss 2.5120\n",
      "step 3600: train loss 2.4886, val loss 2.4972\n",
      "step 3800: train loss 2.4917, val loss 2.5189\n",
      "step 4000: train loss 2.4918, val loss 2.5171\n",
      "step 4200: train loss 2.4862, val loss 2.5090\n",
      "step 4400: train loss 2.4878, val loss 2.4918\n",
      "step 4600: train loss 2.4759, val loss 2.5152\n",
      "step 4800: train loss 2.4921, val loss 2.5143\n"
     ]
    }
   ],
   "source": [
    "mv2 = BigramLanguageModel_v2(vocab_size, block_size, 32)\n",
    "train_model(mv2, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LI s f ve and, he nse le d'dill we,\n",
      "\n",
      "LARlaut yore s unakt he d, tou--we t o ibousry teslounane, m nyoupire, s srgo 's? d de hou thok n nt d.\n",
      "ORGENraceve le stin.\n",
      "Ans fo ar iteanfond:\n",
      "\n",
      "Istee LI fr r, o\n"
     ]
    }
   ],
   "source": [
    "output = mv2.generate(idx = newline, max_new_tokens=200)\n",
    "print(decode(output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
